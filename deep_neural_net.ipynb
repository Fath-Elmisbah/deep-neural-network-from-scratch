{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_neural_net.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOMpHrkPt5MQGtnOC2Kt6Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fathelrahman-Elmisbah/deep-neural-network-from-scratch/blob/master/deep_neural_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhlH4hrtY-tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_OHgaCp1mLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(dims, initialization_ratio):\n",
        "  dims = np.array(dims).reshape(-1,1)\n",
        "  L = dims.shape[0] \n",
        "  parameters = {}\n",
        "  for i in range(1,L):\n",
        "    dim = dims[i,0]\n",
        "    pdim = dims[i-1,0]\n",
        "    W = np.random.randn(dim,pdim) * initialization_ratio\n",
        "    b = np.zeros((dim,1))\n",
        "    parameters['W'+str(i)] = W\n",
        "    parameters['b'+str(i)] = b\n",
        "  return parameters   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta01CRk0QFhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test initialize_parameters\n",
        "\n",
        "dims = [2,3,1]\n",
        "parameters = initialize_parameters(dims, .01)\n",
        "\n",
        "assert(parameters['W1'].shape == (3,2))\n",
        "assert(parameters['b2'].shape == (1,1))\n",
        "assert(len(parameters) == 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfeNnbPn12uM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_linear_step(A_prev, W, b):\n",
        "  Z = W @ A_prev + b\n",
        "  cache = {}\n",
        "  cache['W'] = W\n",
        "  cache['A_prev'] = A_prev\n",
        "  assert(Z.shape == (W.shape[0],A_prev.shape[1]))\n",
        "  return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0YOuTdbQzfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test forward_linear_step\n",
        "\n",
        "A_prev = np.array([[2,3,1],[-2,-1,0]])\n",
        "W = np.array([[1,1],[0,3]])\n",
        "b = np.array([[-3],[2]]).reshape((2,1))\n",
        "Z, cache = forward_linear_step(A_prev, W, b)\n",
        "\n",
        "assert(Z.shape == (2,3))\n",
        "assert(Z[1,1] == -1)\n",
        "assert(cache['W'][1,1] == 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vphey1l2AUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_activation_step(Z, activation):\n",
        "  if activation == 'relu':\n",
        "    A = np.maximum(Z,0)\n",
        "    cache = Z\n",
        "  elif activation == 'tanh':\n",
        "    A = np.tanh(Z)\n",
        "    cache = A\n",
        "  elif activation == 'sigmoid':\n",
        "    A = 1 / (1 + np.exp(-Z))\n",
        "    cache = A\n",
        "  assert(A.shape == Z.shape)  \n",
        "  return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJVBcxHeSf9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test forward_activation_step\n",
        "\n",
        "Z = np.array([[1],[2],[1]]).reshape((3,1))\n",
        "A, cache = forward_activation_step(Z,'relu')\n",
        "\n",
        "assert(A[1,0] == 2)\n",
        "assert(cache[2,0] == 1)\n",
        "\n",
        "\n",
        "A, cache = forward_activation_step(Z,'sigmoid')\n",
        "\n",
        "assert(abs(A[2,0] - .7310585) < .0000001)\n",
        "assert(cache[0,0] == A[0,0])\n",
        "\n",
        "\n",
        "A, cache = forward_activation_step(Z,'tanh')\n",
        "assert(abs(A[0,0] - .76159416) < .0000001)\n",
        "assert(cache[2,0] == A[2,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCNecfDG2c3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_step(A_prev, W, b, activation):\n",
        "  Z, linear_cache = forward_linear_step(A_prev, W, b)\n",
        "  A, activation_cache = forward_activation_step(Z, activation)\n",
        "  cache = (linear_cache, activation_cache)\n",
        "  assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "  return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MSQqbeJTS-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test forward_step\n",
        "\n",
        "A_prev = np.array([[2,3,1],[-2,-1,0]])\n",
        "W = np.array([[1,1],[0,3]])\n",
        "b = np.array([[-3],[2]]).reshape((2,1))\n",
        "A, cache = forward_step(A_prev, W, b, 'relu')\n",
        "linear_cache, activation_cache = cache\n",
        "\n",
        "assert(A.shape == (2,3))\n",
        "assert(A[1,1] == 0)\n",
        "assert(A[1,2] == 2)\n",
        "assert(linear_cache['W'][1,1] == 3)\n",
        "assert(activation_cache[1,1] == -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVlJBTk-2n0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(X, parameters, activations):\n",
        "  L = len(parameters) // 2\n",
        "  caches = []\n",
        "  A = X\n",
        "  if len(activations) == 0:\n",
        "    activations = ['relu'] * (L - 1) + ['sigmoid']\n",
        "  for i in range(1,L + 1):\n",
        "    A_prev = A\n",
        "    W = parameters['W'+str(i)]\n",
        "    b = parameters['b'+str(i)]\n",
        "    activation = activations[i-1]\n",
        "    A, cache = forward_step(A_prev, W, b, activation)\n",
        "    caches.append(cache)\n",
        "  AL = A  \n",
        "  assert(AL.shape == (parameters['W'+str(L)].shape[0],X.shape[1]))  \n",
        "  return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ3KavyHU-Df",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test forward_propagation\n",
        "\n",
        "X = np.array([[2,3,1],[-2,-1,0]])\n",
        "W1 = np.array([[1,1],[0,3]])\n",
        "b1 = np.array([[-3],[2]]).reshape((2,1))\n",
        "W2 = np.array([2,1]).reshape((1,2))\n",
        "b2 = np.array([[5]]).reshape((1,1))\n",
        "parameters = {'W1' : W1, 'b1' : b1, 'W2' : W2, 'b2' : b2}\n",
        "AL, caches = forward_propagation(X, parameters, [])\n",
        "linear_cache1, activation_cache1 = caches[0]\n",
        "linear_cache2, activation_cache2 = caches[1]\n",
        "\n",
        "assert(AL.shape == (1,3))\n",
        "assert(linear_cache1['W'].shape == (2,2))\n",
        "assert(activation_cache1.shape == (2,3))\n",
        "assert(activation_cache2.shape == (1,3))\n",
        "assert(abs(AL[0,2] - .99908895) < .00000001)\n",
        "assert(abs(AL[0,0] - .99330715) < .00000001)\n",
        "assert(activation_cache1[1,0] == -4)\n",
        "assert(activation_cache2[0,2] == AL[0,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk6u3QII2y7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_cost(Y, AL):\n",
        "  cost = -np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL)) / Y.shape[1]\n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKG173CYXv7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test log_cost\n",
        "\n",
        "AL = np.array([.8,.2]).reshape((1,2))\n",
        "Y = np.array([0,1]).reshape((1,2))\n",
        "cYAL = log_cost(Y, AL)\n",
        "assert(abs(cYAL - 1.6094379) < .0000001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0X8YUQ4E6Nq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cca9718b-7e54-40c8-e307-ed3b8611c0b5"
      },
      "source": [
        "def grad_log_cost(Y, AL):\n",
        "  dAL = (1 - Y) / (1 - AL) + Y / AL\n",
        "  assert(dAL.shape, AL.shape)\n",
        "  return dAL"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-42-ba5ec520f9c3>:3: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "  assert(dAL.shape, AL.shape)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIqj-Y3J3GrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_linear_step(dZ, cache):\n",
        "  W = cache['W']\n",
        "  A_prev = cache['A_prev']\n",
        "  m = dZ.shape[1]\n",
        "  dA_prev = W.T @ dZ\n",
        "  dW = (dZ @ A_prev.T) / m\n",
        "  db = np.sum(dZ, axis = 1, keepdims = True) / m\n",
        "  assert(dA_prev.shape == A_prev.shape)\n",
        "  assert(dW.shape == W.shape)\n",
        "  assert(db.shape == (W.shape[0],1))\n",
        "  return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pc3YKN-aFee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ca034738-b6a0-4d97-f8d6-e2fcdba855f0"
      },
      "source": [
        "#test backward_linear_step\n",
        "\n",
        "alpha = .00001\n",
        "beta = .0001\n",
        "A_prev = np.array([[2,3.,1],[-2,-1,0]])\n",
        "W = np.array([[1,1],[0,3.]])\n",
        "b = np.array([[-3],[2.]]).reshape((2,1))\n",
        "Z, cache = forward_linear_step(A_prev, W, b)\n",
        "\n",
        "Z2 = np.copy(Z)\n",
        "Z2[0,0] = Z2[0,0] + alpha\n",
        "Y = np.sum(np.sum(Z))\n",
        "Y2 = np.sum(np.sum(Z2))\n",
        "dZ = ((Y2 - Y) / alpha) * np.ones(Z.shape)\n",
        "\n",
        "W2 = np.copy(W)\n",
        "W2[1,1] += alpha\n",
        "Z2,_ = forward_linear_step(A_prev, W2, b)\n",
        "Y2 = np.sum(np.sum(Z2))\n",
        "dW11 = (Y2 - Y) / alpha / 3\n",
        "\n",
        "b2 = np.copy(b)\n",
        "b2[1,0] += alpha\n",
        "Z2,_ = forward_linear_step(A_prev, W, b2)\n",
        "Y2 = np.sum(np.sum(Z2))\n",
        "db10 = (Y2 - Y) / alpha / 3\n",
        "\n",
        "A_prev2 = np.copy(A_prev)\n",
        "A_prev2[0,1] = A_prev2[0,1] + alpha\n",
        "Z2,_ = forward_linear_step(A_prev2, W, b)\n",
        "Y2 = np.sum(np.sum(Z2))\n",
        "dA_prev01 = (Y2 - Y) / alpha\n",
        "\n",
        "dA_prev, dW, db = backward_linear_step(dZ, cache)\n",
        "\n",
        "assert(abs(dW11 - dW[1,1]) < beta)\n",
        "assert(abs(db10 - db[1,0]) < beta)\n",
        "assert(abs(dA_prev01 - dA_prev[0,1]) < beta)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9999999999621422\n",
            "0.9999999999621422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBoBuJdo3Q5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_activation_step(dA, cache, activation):\n",
        "  if activation == 'relu':\n",
        "    Z = np.copy(cache)\n",
        "    Z[Z>=0] = 1\n",
        "    Z[Z<0] = 0\n",
        "    dZ = dA * Z\n",
        "  elif activation == 'tanh':\n",
        "    A = cache\n",
        "    dZ = dA * (1 - A ** 2)\n",
        "  elif activation == 'sigmoid':\n",
        "    A = cache\n",
        "    dZ = dA * A * (1 - A)\n",
        "  assert(dZ.shape == dA.shape)  \n",
        "  return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BEGAvQycjgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test backward_activation_step\n",
        "\n",
        "alpha = .0000000001\n",
        "beta = .000000001\n",
        "Z = np.array([[1,2.,2],[3,2,1]])\n",
        "A, cache = forward_activation_step(Z, 'relu')\n",
        "\n",
        "A2 = np.copy(A)\n",
        "A2[0,0] += alpha\n",
        "Y = np.sum(np.sum(A))\n",
        "Y2 = np.sum(np.sum(A2))\n",
        "dA = (Y2 - Y) / alpha * np.ones(A.shape)\n",
        "\n",
        "Z2 = np.copy(Z)\n",
        "Z2[1,1] += alpha\n",
        "A2, _ = forward_activation_step(Z2, 'relu')\n",
        "Y2 = np.sum(np.sum(A2))\n",
        "dZ11 = (Y2 - Y) / alpha\n",
        "\n",
        "dZ = backward_activation_step(dA, cache, 'relu')\n",
        "\n",
        "assert(abs(dZ11 - dZ[1,1]) < beta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V-eq3Gz3Yzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_step(dA, cache, activation):\n",
        "  linear_cache, activation_cache = cache\n",
        "  dZ = backward_activation_step(dA, activation_cache, activation)\n",
        "  dA_prev, dW, db = backward_linear_step(dZ, linear_cache)\n",
        "  assert(dA_prev.shape == linear_cache['A_prev'].shape)\n",
        "  assert(dW.shape == linear_cache['W'].shape)\n",
        "  assert(db.shape == (linear_cache['W'].shape[0],1))\n",
        "  return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFIvZRAzdQN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test backward_step\n",
        "\n",
        "alpha = .000001\n",
        "beta = .00001\n",
        "A_prev = np.array([[2,3.,1],[-2,-1,0]])\n",
        "W = np.array([[1.,1.],[0,3.]])\n",
        "b = np.array([[-3],[2.]]).reshape((2,1))\n",
        "A, cache = forward_step(A_prev, W, b, 'sigmoid')\n",
        "\n",
        "A2 = np.copy(A)\n",
        "A2[0,0] += alpha\n",
        "Y = np.sum(np.sum(A))\n",
        "Y2 = np.sum(np.sum(A2))\n",
        "dA = (Y2 - Y) / alpha * np.ones(A.shape)\n",
        "\n",
        "W2 = np.copy(W)\n",
        "W2[1,1] = W2[1,1] + alpha\n",
        "A2, _ = forward_step(A_prev, W2, b, 'sigmoid')\n",
        "Y2 = np.sum(np.sum(A2))\n",
        "dW11 = (Y2 - Y) / alpha / 3\n",
        "\n",
        "b2 = np.copy(b)\n",
        "b2[1,0] += alpha\n",
        "A2, _ = forward_step(A_prev, W, b2, 'sigmoid')\n",
        "Y2 = np.sum(np.sum(A2))\n",
        "db10 = (Y2 - Y) / alpha / 3\n",
        "\n",
        "A_prev2 = np.copy(A_prev)\n",
        "A_prev2[0,1] += alpha\n",
        "A2, _ = forward_step(A_prev2, W, b, 'sigmoid')\n",
        "Y2 = np.sum(np.sum(A2))\n",
        "dA_prev01 = (Y2 - Y) / alpha\n",
        "\n",
        "dA_prev, dW, db = backward_step(dA, cache, 'sigmoid')\n",
        "\n",
        "assert(abs(dW11 - dW[1,1]) < beta)\n",
        "assert(abs(db10 - db[1,0]) < beta)\n",
        "assert(abs(dA_prev01 - dA_prev[0,1]) < beta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0ueSTf43hLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_propagation(Y, AL, caches, activations):\n",
        "  L = len(caches)\n",
        "  if len(activations) == 0:\n",
        "    activations = ['relu'] * (L - 1) + ['sigmoid']\n",
        "  dA_prev = grad_log_cost(Y, AL)\n",
        "  grads = {}\n",
        "  for i in range(L,0,-1):\n",
        "    dA = dA_prev\n",
        "    cache = caches[i-1]\n",
        "    activation = activations[i-1]\n",
        "    dA_prev, dW, db = backward_step(dA, cache, activation)\n",
        "    grads['dW'+str(i)] = dW\n",
        "    grads['db'+str(i)] = db\n",
        "  return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQSkJ3SPdyUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test backward_propagation_step\n",
        "\n",
        "X = np.array([[2.,3,1],[-2,-1,0]])\n",
        "W1 = np.array([[1.,1],[0,3]])\n",
        "b1 = np.array([[-3.],[2]]).reshape((2,1))\n",
        "W2 = np.array([2,1.]).reshape((1,2))\n",
        "b2 = np.array([[5.]]).reshape((1,1))\n",
        "Y = np.array([1,1.,1]).reshape((1,3))\n",
        "parameters = {'W1' : W1, 'b1' : b1, 'W2' : W2, 'b2' : b2}\n",
        "AL, caches = forward_propagation(X, parameters, [])\n",
        "\n",
        "C = log_cost(Y,AL)\n",
        "\n",
        "W1_2 = np.copy(W1)\n",
        "W1_2[1,1] += alpha\n",
        "parameters['W1'] = W1_2\n",
        "AL2, caches = forward_propagation(X, parameters, [])\n",
        "parameters['W1'] = W1\n",
        "C2 = log_cost(Y,AL2)\n",
        "dW1_11 = (C - C2) / alpha\n",
        "\n",
        "W1_2 = np.copy(W1)\n",
        "W1_2[1,0] += alpha\n",
        "parameters['W1'] = W1_2\n",
        "AL2, caches = forward_propagation(X, parameters, [])\n",
        "parameters['W1'] = W1\n",
        "C2 = log_cost(Y,AL2)\n",
        "dW1_10 = (C - C2) / alpha\n",
        "\n",
        "b2_2 = np.copy(b2)\n",
        "b2_2[0,0] += alpha\n",
        "parameters['b2'] = b2_2\n",
        "AL2, caches = forward_propagation(X, parameters, [])\n",
        "parameters['b2'] = b2\n",
        "C2 = log_cost(Y,AL2)\n",
        "db2_00 = (C - C2) / alpha\n",
        "\n",
        "grads = backward_propagation(Y, AL, caches, [])\n",
        "\n",
        "\n",
        "assert(abs(dW1_11 - grads['dW1'][1,1]) < beta)\n",
        "assert(abs(dW1_10 - grads['dW1'][1,0]) < beta)\n",
        "assert(abs(db2_00 - grads['db2'][0,0]) < beta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9kAeN9o3vKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "  L = len(parameters) // 2\n",
        "  new_parameters = {}\n",
        "  for i in range(1,L + 1):\n",
        "    new_parameters['W'+str(i)] = parameters['W'+str(i)] - learning_rate * grads['dW'+str(i)]\n",
        "    new_parameters['b'+str(i)] = parameters['b'+str(i)] - learning_rate * grads['db'+str(i)]\n",
        "  return new_parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBvlhrLWYrzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test update_parameters\n",
        "\n",
        "W1 = np.array([[1.,1],[0,3]])\n",
        "b1 = np.array([[-3.],[2]]).reshape((2,1))\n",
        "W2 = np.array([2,1.]).reshape((1,2))\n",
        "b2 = np.array([[5.]]).reshape((1,1))\n",
        "parameters = {'W1' : W1, 'b1' : b1, 'W2' : W2, 'b2' : b2}\n",
        "dW1 = np.array([[2.,-2],[1,-4]])\n",
        "db1 = np.array([[2.],[1]]).reshape((2,1))\n",
        "dW2 = np.array([1,-3.]).reshape((1,2))\n",
        "db2 = np.array([[2.]]).reshape((1,1))\n",
        "grads = {'dW1' : dW1, 'db1' : db1, 'dW2' : dW2, 'db2' : db2}\n",
        "parameters = update_parameters(parameters, grads, .01)\n",
        "assert(parameters['W1'][0,1] == 1.02)\n",
        "assert(parameters['b2'][0,0] == 4.98)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmmNXWEo31Bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model(X, Y, dims, activations = [], initialization_ratio = .01, learning_rate = .3, number_of_iterations = 1000):\n",
        "  parameters = initialize_parameters(dims, initialization_ratio);\n",
        "  for i in range(number_of_iterations):\n",
        "    AL, caches = forward_propagation(X, parameters, activations)\n",
        "    grads = backward_propagation(Y, AL, caches, activations)\n",
        "    parameters = update_parameters(parameters, grads, learning_rate)\n",
        "    if (i % (number_of_iterations//10) == 0):\n",
        "      cost = grad_log_cost(Y, AL)\n",
        "      print('cost after ' + str(i) + ' iterations: ' + cost)\n",
        "  return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz5Uxz8D4jyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, parameters, activations = []):\n",
        "  A,_ = forward_propagation(X, parameters, activations)\n",
        "  A[A>=.5] = 1\n",
        "  A[A<.5] = 0\n",
        "  return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGBQ8vt54z1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def assess_model(X, Y, parameters, activations):\n",
        "  A,_ = forward_propagation(X, parameters, activations)\n",
        "  accuracy = (A.T @ Y) + ((1 - A).T @ (1 - Y)) / Y.size\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "merYMGABwkmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}